{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shap\n",
    "\n",
    "# progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download from\n",
    "# https://github.com/dynalogic-agtech/dynalogic-agro-met-equations-pkg/tree/14c5f973676152f95005c1ecbcff423e3ea7ce28\n",
    "import fao\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedKFold # train_test_split, RepeatedKFold, TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# XGBoost\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib\n",
    "# # updating the matplotlib configuration parameters\n",
    "# mpl.rcParams.update({'font.size': 20, 'font.family': 'STIXGeneral', 'mathtext.fontset': 'stix'})\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current working directory\n",
    "path = os.getcwd()\n",
    "print('Default path: {}'.format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the csv from folder\n",
    "# Columns: date\ttmax\tSmx\ttmin\tSmn\train\tSrn\tEvap\tSev\tradn\t...\tarea_soybeans\tarea_canola\tarea_corn\tarea_almond\tarea_barley\tarea_oats\tarea_sorghum\tarea_oilseeds\tarea_pasture\tnet_diversion\n",
    "# Data not provided here\n",
    "df_actual_weather = pd.read_csv('daily_weather.csv')\n",
    "\n",
    "df_actual_weather.head(-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting date format\n",
    "df_actual_weather['date'] = pd.to_datetime(df_actual_weather['date'], format = '%d/%m/%Y')  ## %d/%m/%Y, %Y%m%d\n",
    "\n",
    "# setting date as index\n",
    "df_actual_weather.set_index('date', inplace = True)\n",
    "\n",
    "df_actual_weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing ETo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assign altitude, latitude, longitude, soil heat flux, and time period for ET calculation\n",
    "altitude = 121\n",
    "latitude_degree = -34.8016\n",
    "soil_heat_flux = 0.0  # soil heat flux MJ/m^2/day default= 0.0\n",
    "time_period = \"daily\"\n",
    "wind_speed = 2  # wind speed m/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting from decimal degrees to radian\n",
    "latitude = math.radians(latitude_degree)\n",
    "\n",
    "for index, row in tqdm(df_actual_weather.iterrows(), total = df_actual_weather.shape[0]):\n",
    "    # print(index, row)\n",
    "    date = index.date()\n",
    "    day_of_year = pd.to_datetime(date).dayofyear\n",
    "    avp = df_actual_weather.loc[index, 'vp']/10 # kPa\n",
    "    temperature_min = df_actual_weather.loc[index, 'tmin']  # oC\n",
    "    temperature_max = df_actual_weather.loc[index, 'tmax'] # oC\n",
    "    solar_radiation = df_actual_weather.loc[index, 'radn']  # MJ/m^2/day\n",
    "    # wind_speed_3m =df_actual_weather.loc[index, 'wind_2m'] # km/h\n",
    "    \n",
    "\n",
    "    # Clear sky radiation\n",
    "    solar_declination = fao.sol_dec(day_of_year)\n",
    "    sunset_hour_angle = fao.sunset_hour_angle(latitude, solar_declination)\n",
    "    inverse_relative_distance = fao.inv_rel_dist_earth_sun(day_of_year)\n",
    "    extraterrestrial_radiation = fao.et_rad(latitude, solar_declination, sunset_hour_angle, inverse_relative_distance)\n",
    "    clear_sky_radiation = fao.cs_rad(altitude, extraterrestrial_radiation)\n",
    "    \n",
    "    # Degree to Kelvin\n",
    "    temperature_min_kelvin = temperature_min + 273\n",
    "    temperature_max_kelvin = temperature_max + 273\n",
    "    \n",
    "    # Net outgoing long-wave, net incoming short-wave, net total radiations\n",
    "    net_outgoing_longwave_radiation = fao.net_out_lw_rad(temperature_min_kelvin, temperature_max_kelvin, solar_radiation,\n",
    "                                                         clear_sky_radiation, avp)\n",
    "    net_incoming_shortwave_radiation = fao.net_in_sol_rad(solar_radiation, albedo=0.23)\n",
    "    net_radiation = fao.net_rad(net_incoming_shortwave_radiation, net_outgoing_longwave_radiation)\n",
    "    \n",
    "    # Saturation vapour pressure\n",
    "    saturation_vp_max = fao.svp_from_t(temperature_max)\n",
    "    saturation_vp_min = fao.svp_from_t(temperature_min)\n",
    "    sat_vp = fao.svp(saturation_vp_min, saturation_vp_max)\n",
    "    \n",
    "    temperature_mean = (temperature_min + temperature_max) / 2\n",
    "    \n",
    "    # Parameters\n",
    "    latent_heat = fao.latent_heat(temperature_mean)\n",
    "    delta_saturation_vp = fao.delta_svp(temperature_mean)\n",
    "    # wind_speed = fao.wind_speed_2m(wind_speed_3m, 3)\n",
    "    \n",
    "    atmosphere_pressure = fao.atm_pressure(altitude)\n",
    "    psy = fao.psy_const(atmosphere_pressure, latent_heat)\n",
    "    \n",
    "    eto = fao.fao56_penman_monteith(net_radiation, temperature_mean, wind_speed, latent_heat, sat_vp, avp,\n",
    "                                    delta_saturation_vp, psy, soil_heat_flux, time_period)  # -> mm/day\n",
    "    # print(date, eto)\n",
    "\n",
    "    df_actual_weather.loc[index, 'eto'] = round(eto, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop Irrigation Requirement\n",
    "\n",
    "    crop_evapotranspiration(etc) = crop_coefficients (kc)* reference evapotranspiration\n",
    "    cwr = [{(etc (mm) - daily_eff_rain (mm))/1000000 (converting to km)] * Area (km2)}] * 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing crop evapotranspiration (ETc) for each crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_weather['etc_rice'] = df_actual_weather['kc_rice'] * df_actual_weather['eto']\n",
    "df_actual_weather['etc_cotton'] = df_actual_weather['kc_cotton'] * df_actual_weather['eto']\n",
    "df_actual_weather['etc_winter_wheat'] = df_actual_weather['kc_winter_wheat'] * df_actual_weather['eto']\n",
    "df_actual_weather['etc_soybeans'] = df_actual_weather['kc_soybeans'] * df_actual_weather['eto']\n",
    "df_actual_weather['etc_canola'] = df_actual_weather['kc_canola'] * df_actual_weather['eto']\n",
    "df_actual_weather['etc_corn'] = df_actual_weather['kc_corn'] * df_actual_weather['eto']\n",
    "df_actual_weather['etc_almond'] = df_actual_weather['kc_almond'] * df_actual_weather['eto']\n",
    "df_actual_weather['etc_barley'] = df_actual_weather['kc_barley'] * df_actual_weather['eto']\n",
    "df_actual_weather['etc_oats'] = df_actual_weather['kc_oats'] * df_actual_weather['eto']\n",
    "df_actual_weather['etc_sorghum'] = df_actual_weather['kc_sorghum'] * df_actual_weather['eto']\n",
    "df_actual_weather['etc_oilseeds'] = df_actual_weather['kc_oilseeds'] * df_actual_weather['eto']\n",
    "df_actual_weather['etc_pasture'] = df_actual_weather['kc_pasture'] * df_actual_weather['eto']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soil water balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1\n",
    "* Other assumptions\n",
    "  * max soil moisture (s_max) = 100 mm\n",
    "  * soil moisture threshold for irrigation (s_o) = 50 mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runoff\n",
    "q = 0\n",
    "# maximum soil moisture (100)\n",
    "s_max = 100\n",
    "# soil moisture threshold for irrigation (s_o < 50 implies irrigation is required) (s_o = s_max/2)\n",
    "s_o = s_max/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops_list = ['rice', 'cotton', 'winter_wheat', 'soybeans', 'canola', 'corn', 'almond', 'barley', 'oats', 'sorghum', 'oilseeds', 'pasture']\n",
    "\n",
    "# # total irrigation [case 1]\n",
    "# df_cwr_rainfall['c1_total_irr'] = 0\n",
    "\n",
    "# assuming the soil moisture equivalent to s_o \n",
    "s_initial_dict = dict()\n",
    "for crop in crops_list:\n",
    "    s_initial_dict[crop] = s_o\n",
    "\n",
    "for index, row in df_actual_weather.iterrows(): # df_cwr_rainfall.iloc[595:].head(10)\n",
    "    # print(index, row['rain'])\n",
    "\n",
    "    total_irrigation = 0\n",
    "\n",
    "    for crop in crops_list:\n",
    "        area_column = 'area_' + crop\n",
    "        etc_column = 'etc_' + crop\n",
    "        s_column = 'c1_s_' + crop\n",
    "        # irr_column = 'c1_irr_' + crop\n",
    "        irr_column = 'CIR_' + crop\n",
    "        runoff_column = 'c1_ro_' + crop\n",
    "        # print('\\t', crop, etc_column, s_column, irr_column)\n",
    "\n",
    "        # computing crop specific soil moisture storage\n",
    "        s_crop = (s_initial_dict[crop] + row['rain'] - row[etc_column]).round(3)\n",
    "        \n",
    "        # crop specific soil moisture greater than maximum soil moisture capacity \n",
    "        if s_crop > s_max:\n",
    "            # computing runoff (assumption: no infiltration)\n",
    "            runoff_crop = s_crop - s_max\n",
    "            # irrigation\n",
    "            irr_crop = 0\n",
    "            # setting crop specific soil moisture equivalent to the maximum soil moisture capacity\n",
    "            s_crop = s_max\n",
    "\n",
    "            # updating dataframe\n",
    "            df_actual_weather.loc[index, s_column] = s_crop\n",
    "            df_actual_weather.loc[index, irr_column] = irr_crop\n",
    "            df_actual_weather.loc[index, runoff_column] = runoff_crop\n",
    "\n",
    "        # crop specific soil moisture between soil moisture threshold and maximum soil moisture capacity\n",
    "        elif s_max/2 <= s_crop <= s_max:\n",
    "            # runoff\n",
    "            runoff_crop = 0\n",
    "            # irrigation\n",
    "            irr_crop = 0\n",
    "\n",
    "            # updating dataframe\n",
    "            df_actual_weather.loc[index, s_column] = s_crop\n",
    "            df_actual_weather.loc[index, irr_column] = irr_crop\n",
    "            df_actual_weather.loc[index, runoff_column] = runoff_crop\n",
    "            \n",
    "        # crop specific soil moisture less than soil moisture threshold\n",
    "        else:\n",
    "            # runoff\n",
    "            runoff_crop = 0\n",
    "            # irrigation\n",
    "            irr_crop = s_max/2 - s_crop \n",
    "            # after irrigation s_crop will be equivalent to s_o (s_max/2)\n",
    "            s_crop = s_max/2\n",
    "\n",
    "            # updating dataframe\n",
    "            df_actual_weather.loc[index, s_column] = s_crop\n",
    "            df_actual_weather.loc[index, irr_column] = irr_crop\n",
    "            df_actual_weather.loc[index, runoff_column] = runoff_crop\n",
    "        \n",
    "        # updating s_initial_dict\n",
    "        s_initial_dict[crop] = s_crop\n",
    "\n",
    "        # adding up irrigation to get total irrigation\n",
    "        total_irrigation += (irr_crop/1000000 * row[area_column]*1000000).round(3)\n",
    "    \n",
    "    df_actual_weather.loc[index, 'CIR_total'] = total_irrigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of CIR vs Actual Demand (ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8, 6))\n",
    "\n",
    "# df_cwr_rainfall.plot(y = 'daily_cwr', ax = ax, legend = True, label = 'Daily CWR (ML)') # color = 'blue'\n",
    "df_actual_weather.plot(y = 'CIR_total', ax = ax, legend = True, label = 'Daily CIR (ML)', color = 'green', fontsize= 10) # color = 'blue'\n",
    "df_actual_weather.plot(y = 'net_diversion', ax = ax, legend = True, label = 'Actual Demand (ML)', color = 'orange', fontsize= 10)\n",
    "# df_cwr_rainfall.plot(y = 'rain', ax = ax, legend = True, label = 'Rain (mm)', color = 'red')\n",
    "\n",
    "# ax.set_ylim([0, 6000])\n",
    "\n",
    "plt.title('Comparison Daily Actual Demand and CWN')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Water Demand (ML)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rain distribution throughout the year from 2018 to 2024\n",
    "fig, ax = plt.subplots(figsize = (8, 5))\n",
    "df_actual_weather.plot(y = 'rain', ax = ax, legend = True, label = 'Rain (mm)', color = 'red')\n",
    "\n",
    "# ax.set_ylim([0, 6000])\n",
    "\n",
    "plt.title('Rain')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily rain (mm)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the data in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_weather.to_csv('./weather_and_CIR.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-driven Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading csv file from the directory\n",
    "\n",
    "df_diversion = pd.read_csv('weather_and_CIR.csv')\n",
    "\n",
    "\n",
    "df_diversion['date'] = pd.to_datetime(df_diversion['date'], format = '%Y-%m-%d')\n",
    "# df_diversion['date'] = pd.to_datetime(df_diversion['date'], format = '%d/%m/%Y')\n",
    "# setting date as index\n",
    "df_diversion.set_index('date', inplace = True)\n",
    "\n",
    "df_diversion.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the features\n",
    "\n",
    "7 days lead and 7 days lag input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lag_days = 7\n",
    "\n",
    "feature_list = ['tmin', 'tmax', 'rain', 'eto', 'radn', 'CIR_total', 'net_diversion']\n",
    "\n",
    "df_features = pd.DataFrame(data = None)\n",
    "\n",
    "for index, row in df_diversion[feature_list].iterrows():\n",
    "\n",
    "    ######################### lag part (t-1, ..., t-7) #########################\n",
    "    lag_start_date = index - timedelta(days = max_lag_days)\n",
    "    lag_end_date = index - timedelta(days = 1)\n",
    "    # print(index, lag_start_date, lag_end_date)\n",
    "\n",
    "    temp_df = df_diversion.loc[lag_start_date:lag_end_date]\n",
    "\n",
    "    if len(temp_df) == 0: # len(temp_df) != max_lag_days\n",
    "        # print('Insufficient data')\n",
    "        counter = max_lag_days\n",
    "        for c in range(max_lag_days, 0, -1):\n",
    "            for feature in feature_list:\n",
    "                new_feat_name = feature + '_m{}'.format(c)\n",
    "                df_features.loc[index, new_feat_name] = np.NaN\n",
    "\n",
    "        # continue # for looping\n",
    "    else:\n",
    "        # print('Sufficient data')\n",
    "        counter = len(temp_df)\n",
    "        for index1, row1 in temp_df.iterrows():\n",
    "            for feature in feature_list:\n",
    "                new_feat_name = feature + '_m{}'.format(counter) \n",
    "                df_features.loc[index, new_feat_name] = row1[feature]\n",
    "            \n",
    "            counter = counter - 1\n",
    "\n",
    "\n",
    "    ######################### t #########################\n",
    "    for feature in feature_list:\n",
    "        new_feat_name = feature + '_0'\n",
    "        df_features.loc[index, new_feat_name] = row[feature]\n",
    "    \n",
    "    \n",
    "    ######################### forecast part (t-1, ..., t-7) #########################\n",
    "    forecast_start_date = index + timedelta(days = 1)\n",
    "    forecast_end_date = index + timedelta(days = max_lag_days)\n",
    "    # print(index, forecast_start_date, forecast_end_date)\n",
    "\n",
    "    temp_df = df_diversion.loc[forecast_start_date:forecast_end_date]\n",
    "\n",
    "    if len(temp_df) == 0: # len(temp_df) != max_lag_days\n",
    "        # print('Insufficient data')\n",
    "        counter = max_lag_days\n",
    "        for c in range(1, counter + 1):\n",
    "            for feature in feature_list:\n",
    "                new_feat_name = feature + '_p{}'.format(c) \n",
    "                df_features.loc[index, new_feat_name] = np.NaN\n",
    "        # continue # for looping\n",
    "    else:\n",
    "        # print('Sufficient data')\n",
    "        counter = 1\n",
    "        for index1, row1 in temp_df.iterrows():\n",
    "            for feature in feature_list:\n",
    "                new_feat_name = feature + '_p{}'.format(counter) \n",
    "                df_features.loc[index, new_feat_name] = row1[feature]\n",
    "            \n",
    "            counter = counter + 1\n",
    "\n",
    "print(df_features.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the features as csv\n",
    "df_features.to_csv('./feature_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All required dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with actual diversion\n",
    "csv_diversion = 'weather_and_CIR.csv'\n",
    "\n",
    "# feature data\n",
    "csv_feature = 'feature_dataset.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature = pd.read_csv(csv_feature)\n",
    "df_feature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename the first column as date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature.rename(columns = {'Unnamed: 0': 'date'}, inplace = True)\n",
    "df_feature.to_csv(csv_feature, header = True, index = False, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting date as index\n",
    "df_feature['date'] = pd.to_datetime(df_feature['date'], format = '%Y-%m-%d')\n",
    "\n",
    "# setting date as index\n",
    "df_feature.set_index('date', inplace = True)\n",
    "\n",
    "df_feature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding actual diversion column from diversion dataframe to feature dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversion_column = 'net_diversion'\n",
    "if not diversion_column in df_feature.columns:\n",
    "    df_feature = df_feature.merge(df_diversion['net_diversion'], how = 'inner', on = 'date')\n",
    "\n",
    "df_feature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering data for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 2019\n",
    "end_year = 2022\n",
    "\n",
    "df_feature_subset = df_feature[(df_feature.index.year >= start_year) & (df_feature.index.year <= end_year)]\n",
    "df_feature_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave one year out validation\n",
    "* K-fold Cross-Validation\n",
    "\n",
    "  ![k-fold cross-validation](https://miro.medium.com/v2/resize:fit:720/format:webp/1*RWjVdxk-pcdoGa02t8MLWg.png)\n",
    "\n",
    "* Time Series Split Cross-Validation\n",
    "\n",
    "  ![time series split cross-validation](https://miro.medium.com/v2/resize:fit:640/format:webp/1*XcqvKVTQ6U_zszSD52lSqA.png)\n",
    "\n",
    "* Blocked Cross-Validation\n",
    "\n",
    "  ![blocked cross-validation](https://miro.medium.com/v2/resize:fit:640/format:webp/1*QJaeOqGfe_vKbpmT882APA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed =seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting feature based on lead and lag days based on the defined criterion\n",
    "\n",
    "* m = 7 lag days inputs\n",
    "* p = 7 lead days inputs\n",
    "* mp = combining 7 days lead and lag inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_variables(col_list, filter_col_list, criterion = 'all'):\n",
    "    '''\n",
    "    criterion: 'all', 'm', 'p', 'mp'           \n",
    "    '''\n",
    "    var_list = list()\n",
    "    for fc in filter_col_list:\n",
    "        if criterion == 'all':\n",
    "            l = [col for col in col_list if col.startswith(fc)]\n",
    "            var_list += l\n",
    "\n",
    "        elif criterion == 'm':\n",
    "            fc = fc + '_m'\n",
    "            l = [col for col in col_list if col.startswith(fc)]\n",
    "            var_list += l\n",
    "\n",
    "        elif criterion == 'p':\n",
    "            fc = fc + '_p'\n",
    "            l = [col for col in col_list if col.startswith(fc)]\n",
    "            var_list += l\n",
    "\n",
    "        elif criterion == 'mp':\n",
    "            fc1 = fc + '_m'\n",
    "            fc2 = fc + '_p'\n",
    "            l = [col for col in col_list if (col.startswith(fc1) or col.startswith(fc2))]\n",
    "            var_list += l\n",
    "\n",
    "    return var_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "The most commonly configured hyperparameters are the following:\n",
    "* *n_estimators*: The number of trees in the ensemble, often increased until no further improvements are seen.\n",
    "* *max_depth*: The maximum depth of each tree, often values are between 1 and 10.\n",
    "* *learning_rate*: The learning rate used to weight each model, often set to small values such as 0.3, 0.1, 0.01, or smaller.\n",
    "* *subsample*: The number of samples (rows) used in each tree, set to a value between 0 and 1, often 1.0 to use all samples.\n",
    "* *colsample_bytree*: Number of features (columns) used in each tree, set to a value between 0 and 1, often 1.0 to use all features.\n",
    "\n",
    "https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_vars = ['tmin', 'tmax', 'rain', 'radn', 'eto']\n",
    "other_vars = ['CIR_total', 'net_diversion']\n",
    "\n",
    "w_col = select_variables(list(df_feature_subset.columns), weather_vars, criterion = 'm') # criterion: 'all', 'm', 'p', 'mp'\n",
    "o_col = select_variables(list(df_feature_subset.columns), other_vars, criterion = 'm') # criterion: 'all', 'm','p', 'mp'\n",
    "\n",
    "year_list = list(range(start_year, end_year + 1))\n",
    "# print(year_list)\n",
    "\n",
    "rmse_list = list()\n",
    "nse_list = list()\n",
    "acc_list = list()\n",
    "msss_list = list()\n",
    "r2_list = list()\n",
    "\n",
    "for year in year_list:\n",
    "    print('Train: {} | Test: {}'.format('--', year))\n",
    "    # training set\n",
    "    x_train = df_feature_subset[df_feature_subset.index.year != year][w_col + o_col]\n",
    "    y_train = df_feature_subset[df_feature_subset.index.year != year][diversion_column]\n",
    "\n",
    "    # testing set\n",
    "    x_test = df_feature_subset[df_feature_subset.index.year == year][w_col + o_col]\n",
    "    y_test = df_feature_subset[df_feature_subset.index.year == year][diversion_column]\n",
    "\n",
    "    # print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "    # creating an xgboost regression model\n",
    "    # xgb_model = XGBRegressor(random_state = seed)\n",
    "    xgb_model = XGBRegressor(n_estimators = 100, max_depth = 7, learning_rate = 0.1, subsample = 0.7, colsample_bytree = 0.8, random_state = seed)\n",
    "\n",
    "    # fitting the model\n",
    "    xgb_model.fit(x_train, y_train)\n",
    "\n",
    "    # predicting on test set\n",
    "    y_pred = xgb_model.predict(x_test)\n",
    "\n",
    "    ## evaluating the model\n",
    "    # rmse\n",
    "    rmse = round(mean_squared_error(y_test, y_pred, squared = False), 3)\n",
    "\n",
    "    # nash-sutcliffe efficienty (NSE) - equivalent to the coefficient of determination\n",
    "    nse = round(1 - np.sum(np.power(y_pred - y_test, 2))/np.sum((np.power(y_test - np.mean(y_test), 2))), 4)\n",
    "\n",
    "    # anomaly correlation coefficient (ACC)\n",
    "    # sharpness of forecasting | ability to forecast extreme values\n",
    "    mean_monthly_diversion = np.mean(df_feature_subset[diversion_column])\n",
    "    acc = round(np.sum((y_pred - mean_monthly_diversion)*(y_test - mean_monthly_diversion))/np.sqrt(np.sum(np.power(y_pred - mean_monthly_diversion, 2))*np.sum(np.power(y_test - mean_monthly_diversion, 2))), 4)\n",
    "\n",
    "    # mean square skill score (MSSS)\n",
    "    # shows the forecaset skill by quantifying accuracy relative to the long-term monthly averged irrigation demand\n",
    "    # value ranges from 0 to infinity\n",
    "    # a good model will have an MSSS value closer to zero\n",
    "    n = len(x_test)\n",
    "    msss = round(1 - np.sum(np.power(y_pred - mean_monthly_diversion, 2))/((n/(n-1))*np.sum(np.power(y_test - mean_monthly_diversion, 2))), 4)\n",
    "\n",
    "    # coefficient of determination\n",
    "    r2 = round(r2_score(y_test, y_pred), 4)\n",
    "    print('\\tRMSE: {} | NSE: {} | ACC: {} | MSSS: {} | R-squared: {}'.format(rmse, nse, acc, msss, r2))\n",
    "\n",
    "    rmse_list.append(rmse)\n",
    "    nse_list.append(nse)\n",
    "    acc_list.append(acc)\n",
    "    msss_list.append(msss)\n",
    "    r2_list.append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('''\\nRMSE: {} \\u00B1 {} | NSE: {} \\u00B1 {} | ACC: {} \\u00B1 {} | MSSS: {} \\u00B1 {} | R-squared: {} \\u00B1 {}'''.format(np.round(np.nanmean(rmse_list), 3), np.round(np.nanstd(rmse_list), 3),\n",
    "                                                                                                                               np.round(np.nanmean(nse_list), 4), np.round(np.nanstd(nse_list), 4),\n",
    "                                                                                                                               np.round(np.nanmean(acc_list), 4), np.round(np.nanstd(acc_list), 4),\n",
    "                                                                                                                               np.round(np.nanmean(msss_list), 4), np.round(np.nanstd(msss_list), 4),\n",
    "                                                                                                                               np.round(np.nanmean(r2_list), 4), np.round(np.nanstd(r2_list), 4),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom cross-validation\n",
    "* https://nander.cc/writing-custom-cross-validation-methods-grid-search\n",
    "* https://stackoverflow.com/questions/30040597/how-to-generate-a-custom-cross-validation-generator-in-scikit-learn\n",
    "* https://domino.ai/blog/guide-to-building-models-with-cross-validation\n",
    "* https://domino.ai/blog/guide-to-building-models-with-cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCrossValidation:\n",
    "    '''\n",
    "    Return the index of train and test set\n",
    "    '''\n",
    "    def __init__(self, start_year, end_year):\n",
    "        self.start_year = start_year\n",
    "        self.end_year = end_year\n",
    "        self.n_splits = self.end_year - self.start_year + 1\n",
    "\n",
    "    def split(self, X, y, groups = None):\n",
    "        year_list = list(range(self.start_year, self.end_year + 1))\n",
    "        for year in year_list:\n",
    "            # training set index\n",
    "            X_ = X.copy()\n",
    "            X_['year'] = X_.index.year\n",
    "            X_ = X_.reset_index(drop = True)\n",
    "            train_index = X_[X_['year'] != year].index\n",
    "\n",
    "            # testing set index\n",
    "            test_index = X_[X_['year'] == year].index\n",
    "\n",
    "            yield train_index, test_index\n",
    "\n",
    "    def get_n_splits(self, X, y, groups=None):\n",
    "        return self.n_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost model\n",
    "* Run only if the training model has to be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_vars = ['tmin', 'tmax', 'rain', 'radn', 'eto']\n",
    "other_vars = ['CIR_total', 'net_diversion']\n",
    "\n",
    "w_col = select_variables(list(df_feature_subset.columns), weather_vars, criterion = 'm') # criterion: 'all', 'm', 'p','mp'\n",
    "o_col = select_variables(list(df_feature_subset.columns), other_vars, criterion = 'm') # criterion: 'all', 'm', 'p', 'mp'\n",
    "\n",
    "n_jobs = 8 # number of cpu\n",
    "xgb_model = XGBRegressor(learning_rate = 0.1, subsample = 1, random_state = seed)\n",
    "\n",
    "param_grid = {'n_estimators': [50, 100, 200], # [50, 100, 200]\n",
    "              'max_depth': [2, 5, 10], # [2, 5, 10]\n",
    "              'subsample': [0.7, 0.8, 0.9, 1.0], # the number of samples used in each tree\n",
    "              'colsample_bytree': [0.7, 0.8, 0.9, 1.0], # number of feature used in each tree\n",
    "              }\n",
    "\n",
    "# splitter for cross-validation\n",
    "# custom_splitter = CustomCrossValidation(start_year, end_year).split(X = df_feature_subset[w_col + o_col], y = df_feature_subset[diversion_column])\n",
    "\n",
    "custom_splitter = CustomCrossValidation(start_year, end_year).split(X = df_feature_subset[w_col + o_col], \n",
    "                                                                    y = df_feature_subset['net_diversion_p7'])\n",
    "\n",
    "# grid search\n",
    "xgb_grid_model = GridSearchCV(xgb_model, param_grid = param_grid, cv = custom_splitter, n_jobs = n_jobs, verbose = 1) # cv = 5\n",
    "\n",
    "\n",
    "xgb_grid_model.fit(df_feature_subset[w_col + o_col], df_feature_subset['net_diversion_p7'])\n",
    "\n",
    "xgb_grid_model_best = xgb_grid_model.best_estimator_\n",
    "\n",
    "print('n_estimators: {} | max_depth: {}'.format(xgb_grid_model_best.n_estimators, xgb_grid_model_best.max_depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model\n",
    "* If the model is retrained, it has to be saved in the folder accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "model_name = './model_lead_7.pkl'\n",
    "print('Output model: {}'.format(model_name))\n",
    "pickle.dump(xgb_grid_model_best, open(model_name, 'wb'))\n",
    "\n",
    "# # # loading model\n",
    "# xgb_grid_model_best = pickle.load(open(model_name, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 days prediction at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rmse(y_true, y_pred):\n",
    "    return round(mean_squared_error(y_true, y_pred, squared = False), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nse(y_true, y_pred):\n",
    "    # nash-sutcliffe efficienty (NSE) - equivalent to the coefficient of determination\n",
    "    return round(1 - np.sum(np.power(y_true - y_pred, 2))/np.sum((np.power(y_true - np.mean(y_true), 2))), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_acc(y_true, y_pred, mean_val):\n",
    "    # anomaly correlation coefficient (ACC)\n",
    "    # sharpness of forecasting | ability to forecast extreme values\n",
    "    return round(np.sum((y_pred - mean_val)*(y_true - mean_val))/np.sqrt(np.sum(np.power(y_pred - mean_val, 2))*np.sum(np.power(y_true - mean_val, 2))), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_msss(y_true, y_pred, mean_val):\n",
    "    # mean square skill score (MSSS)\n",
    "    # shows the forecaset skill by quantifying accuracy relative to the long-term monthly averged irrigation demand\n",
    "    # value ranges from 0 to infinity\n",
    "    # a good model will have an MSSS value closer to zero\n",
    "    n = len(y_true)\n",
    "    return round(1 - np.sum(np.power(y_pred - mean_val, 2))/((n/(n-1))*np.sum(np.power(y_true - mean_val, 2))), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_r2(y_true, y_pred):\n",
    "    # coefficient of determination\n",
    "    return round(r2_score(y_true, y_pred), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_vars = ['tmin', 'tmax', 'rain', 'radn', 'eto']\n",
    "other_vars = ['CIR_total', 'net_diversion']\n",
    "\n",
    "w_col = select_variables(list(df_feature_subset.columns), weather_vars, criterion = 'm') # criterion: 'all', 'm', 'p', 'mp'\n",
    "o_col = select_variables(list(df_feature_subset.columns), other_vars, criterion = 'm') # criterion: 'all', 'm', 'p', 'mp'\n",
    "\n",
    "df_feature_train = df_feature[(df_feature.index.year >=2019) & (df_feature.index.year <= 2022)]\n",
    "df_feature_test = df_feature[df_feature.index.year >= 2023]\n",
    "\n",
    "\n",
    "min_lead_pred = 1\n",
    "max_lead_pred = 7\n",
    "\n",
    "for lead_day in range(min_lead_pred, max_lag_days + 1):\n",
    "    print('Lead day: {}'.format(lead_day))\n",
    "\n",
    "    # loading saved model\n",
    "    model_name = './model_lead_{}.pkl'.format(lead_day)\n",
    "    print('\\tModel name: {}'.format(model_name))\n",
    "\n",
    "    # loading model\n",
    "    model = pickle.load(open(model_name, 'rb'))\n",
    "\n",
    "    actual_diversion_colname = 'net_diversion_p{}'.format(lead_day)\n",
    "    pred_diversion_colname = 'pred_diversion_p{}'.format(lead_day)\n",
    "    print('\\tActual diversion column name: {}'.format(actual_diversion_colname))\n",
    "    print('\\tPredicted diversion column name: {}'.format(pred_diversion_colname))\n",
    "\n",
    "    ################## train set prediction ##################\n",
    "    X_train = df_feature_train[w_col + o_col]\n",
    "    y_train = df_feature_train[actual_diversion_colname]\n",
    "\n",
    "    y_pred = model.predict(X_train)\n",
    "    df_feature_train[pred_diversion_colname] = np.round(y_pred, 2)\n",
    "\n",
    "    # Find indices of NaN values in y_test or y_pred\n",
    "    nan_indices = np.isnan(y_train) | np.isnan(y_pred)\n",
    "\n",
    "    # Filter out NaN values from both y_test and y_pred\n",
    "    y_train = y_train[~nan_indices]\n",
    "    y_pred = y_pred[~nan_indices]\n",
    "    \n",
    "\n",
    "    #### evaluating the model\n",
    "    rmse = evaluate_rmse(y_train, y_pred)\n",
    "    # nash-sutcliffe efficienty (NSE) - equivalent to the coefficient of determination\n",
    "    nse = evaluate_nse(y_train, y_pred)\n",
    "    # anomaly correlation coefficient (ACC)\n",
    "    # sharpness of forecasting | ability to forecast extreme values\n",
    "    mean_monthly_diversion = np.mean(y_train)\n",
    "    acc = evaluate_acc(y_train, y_pred, mean_monthly_diversion)\n",
    "    # mean square skill score (MSSS)\n",
    "    # shows the forecaset skill by quantifying accuracy relative to the long-term monthly averged irrigation demand\n",
    "    # value ranges from 0 to infinity\n",
    "    # a good model will have an MSSS value closer to zero\n",
    "    msss = evaluate_msss(y_train, y_pred, mean_monthly_diversion)\n",
    "    # coefficient of determination\n",
    "    r2 = evaluate_r2(y_train, y_pred)\n",
    "\n",
    "    print('\\tTrain: RMSE: {} | NSE: {} | ACC: {} | MSSS: {} | R-squared: {}'.format(rmse, nse, acc, msss, r2))\n",
    "\n",
    "    ################## train set prediction ##################\n",
    "\n",
    "\n",
    "    ################## test set prediction ##################\n",
    "    X_test = df_feature_test[w_col + o_col]\n",
    "    y_test = df_feature_test[actual_diversion_colname]\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    df_feature_test[pred_diversion_colname] = np.round(y_pred, 2)\n",
    "\n",
    "    # Find indices of NaN values in y_test or y_pred\n",
    "    nan_indices = np.isnan(y_test) | np.isnan(y_pred)\n",
    "\n",
    "    # Filter out NaN values from both y_test and y_pred\n",
    "    y_test = y_test[~nan_indices]\n",
    "    y_pred = y_pred[~nan_indices]\n",
    "\n",
    "\n",
    "    #### evaluating the model\n",
    "    rmse = evaluate_rmse(y_test, y_pred)\n",
    "    # nash-sutcliffe efficienty (NSE) - equivalent to the coefficient of determination\n",
    "    nse = evaluate_nse(y_test, y_pred)\n",
    "    # anomaly correlation coefficient (ACC)\n",
    "    # sharpness of forecasting | ability to forecast extreme values\n",
    "    # mean_monthly_diversion = np.mean(y_train)\n",
    "    acc = evaluate_acc(y_test, y_pred, mean_monthly_diversion)\n",
    "    # mean square skill score (MSSS)\n",
    "    # shows the forecaset skill by quantifying accuracy relative to the long-term monthly averged irrigation demand\n",
    "    # value ranges from 0 to infinity\n",
    "    # a good model will have an MSSS value closer to zero\n",
    "    msss = evaluate_msss(y_test, y_pred, mean_monthly_diversion)\n",
    "    # coefficient of determination\n",
    "    r2 = evaluate_r2(y_test, y_pred)\n",
    "\n",
    "    print('\\tTest: RMSE: {} | NSE: {} | ACC: {} | MSSS: {} | R-squared: {}'.format(rmse, nse, acc, msss, r2))\n",
    "\n",
    "    ################## test set predition ##################\n",
    "\n",
    "    print()\n",
    "\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_feature_test.iloc[-1, -7:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demand_forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
